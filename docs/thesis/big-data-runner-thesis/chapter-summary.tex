\chapter{Podsumowanie} \label{chap.summary}
Celem tego rozdziału jest podsumowanie oraz dyskusja wyników wydajności dostarczonych przez aplikację big-data-runner. Rozdział ten stara się wskazać wyższość jednej z platform Apache Spark oraz Apache Hadoop ze względu na wymagania oraz możliwości końcowego użytkownika. Pierwsze dwa podrozdziały podsumowują wyniki testów wydajności dostarczonych przez aplikację powstałą specjalnie na potrzeby tej pracy magisterskiej. Ostatni podrozdział ocenia możliwość dalszego rozwoju powstałej aplikacji pod względem komercyjnym.   
\section{Dyskusja wyników wydajności operacji dostarczonych przez aplikację big-data-runner}
W tabelach \ref{tab:word-count-results}, \ref{tab:filter-results} oraz \ref{tab:reject-results} zostały przedstawione wyniki wydajności dla badanych operacji: \textit{Word Count}, \textit{Filter}, \textit{Reject}. Dokładny opis operacji można znaleźć w sekcji \ref{sec:user_interfaces}. Można jednoznacznie stwierdzić, że Apache Spark jest zdecydowanie szybszą platformą w stosunku do Apache Hadoop. W przypadku gdy najważniejszym czynnikiem dla końcowego użytkownika jest prędkość analizy danych masowych Apache Spark jest najlepszym rozwiązaniem, zapewni wyniki końcowe w krótszym czasie niż Apache Hadoop. Różnica w wydajności wynika najprawdopodobniej z architektury obydwu platform. Apache Spark korzysta z pamięci RAM - to tam wykonywane są wszelkie obliczenia. Operacje pośrednie są kolekcjonowane w ciąg - \textit{pipeline} i ładowane bezpośrednio do pamięci RAM. Dzięki takiej architekturze obliczeń możliwe jest zaoszczędzenie czasu wykonywania - omijane są operacje wejścia/wyjścia. W przypadku Apache Hadoop każda operacja pośrednia jest zapisywana na dysku co skutkuje wolniejszym dostarczaniem wyników od dwóch i pół do pięciu razy.
\section{Ocena obydwu platform}
Wybór między jednoznaczną wyższością spośród obydwu platform na podstawie wykonanych badań nie jest możliwy. Apache Spark i Apache Hadoop mają wiele zalet jak również wad. Niewątpliwą zaletą Apache Spark jest wydajność, która może być kluczowym aspektem dla wielu użytkowników. Dodatkowym czynnikiem przemawiającym za Apache Spark jest zwięzłość oraz czytelność kodu, która jest ważna wielu projektach informatycznych. Jest to bardzo ważny czynnik szczególnie w środowiskach korporacyjnych, gdzie rotacja osób rozwijających i utrzymujących systemy informatyczne jest częsta a każde niestandardowe bądź mało znane rozwiązane jest uważane za wadę ze względu na trudność w skalowaniu. Jednocześnie jest narzędziem nowym, które jeszcze nie zostało opanowane przez wielu inżynierów co może być problematyczne w początkowej fazie dużego projektu informatycznego, gdzie wymagana jest większa liczba osób pracujących nad systemem niż w fazie utrzymania. Z racji braku wielkiej ilości osób znających Apache Spark ich wynagrodzenie rośnie w górę. Taki stosunek kosztów zatrudniania oraz rekrutacji może być czynnikiem przemawiającym za wybraniem starszej ale lepiej znanej platformy - Apache Hadoop. W przypadku rozwoju aplikacji przeznaczonej na mniejszą skalę bądź na zamówienie klienta zewnętrznego bez zapewnienia utrzymania, warte jest rozważenie czy nie warto zastosować platformy nowszej, mniej znanej lecz zdecydowanie łatwiejszej w rozwijaniu - Apache Spark.
\section{Możliwości rozwoju komercyjnego aplikacji big-data-runner}
Aplikacja big-data-runner może być przydatnym narzędziem dla użytkowników bądź przedsiębiorców zajmujących się przetwarzaniem danych masowych. Aplikacja może jest na tyle elastyczna, że potrafi pobierać dane do testów wydajności z serwisu zewnętrznego jak również wykonywać operacje analizy danych masowych na już wcześniej przygotowanych danych w systemie HDFS. Pod względem komercyjnym mogłaby być rozszerzona o interfejs, który umożliwiałby użytkownikowi przesyłanie jego autorskich programów wsadowych, tak by mógł dokonywać analizy danych na podstawie swoich spersonalizowanych wymagań. Dodatkową funkcjonalnością możliwą do zaimplementowania, która mogłaby być atrakcyjna dla końcowego użytkownika jest automatyczna kalkulacja kosztów najmu chmury obliczeniowej. Taka funkcjonalność mogłaby dać użytkownikowi informację ile kosztuje analiza danych o spersonalizowanych parametrach o zdefiniowanej wielkości oraz jaki jest koszt dostawcy sprzętu na którym jest wdrożona aplikacja. Koszty sprzętowe oraz wyniki wydajności mogą zadecydować o wybraniu jednego z dwóch narzędzi, dostawcy sprzętu komputerowego, bądź dostawcy chmury obliczeniowej.