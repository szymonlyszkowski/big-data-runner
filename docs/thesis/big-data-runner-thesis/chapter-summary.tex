\chapter{Podsumowanie} \label{chap.summary}
Celem tego rozdziału jest podsumowanie oraz dyskusja wyników wydajności dostarczonych przez aplikację big-data-runner. Rozdział ten stara się wskazać wyższość jednej z platform Apache Spark oraz Apache Hadoop ze względu na wymagania oraz możliwości końcowego użytkownika. Pierwsze dwa podrozdziały podsumowują wyniki testów wydajności dostarczonych przez aplikację powstałą specjalnie na potrzeby tej pracy magisterskiej. Ostatni podrozdział ocenia możliwość dalszego rozwoju powstałej aplikacji pod względem komercyjnym.   
\section{Dyskusja wyników wydajności operacji dostarczonych przez aplikację big-data-runner}
W tabelach \ref{tab:word-count-results}, \ref{tab:filter-results} oraz \ref{tab:reject-results} zostały przedstawione wyniki wydajności dla badanych operacji: \textit{Word Count}, \textit{Filter}, \textit{Reject}. Dokładny opis operacji można znaleźć w sekcji \ref{sec:user_interfaces}. Można jednoznacznie stwierdzić, że Apache Spark jest zdecydowanie szybszą platformą w stosunku do Apache Hadoop. W przypadku gdy najważniejszym czynnikiem dla końcowego użytkownika jest prędkość analizy danych masowych Apache Spark jest najlepszym rozwiązaniem, zapewni wyniki końcowe w krótszym czasie niż Apache Hadoop. Różnica w wydajności wynika najprawdopodobniej z architektury obydwu platform. Apache Spark korzysta domyślnie z pamięci RAM - to tam wykonywane są wszelkie obliczenia. Operacje pośrednie są kolekcjonowane w ciąg - \textit{pipeline} i ładowane bezpośrednio do pamięci RAM. Dzięki takiej architekturze obliczeń możliwe jest zaoszczędzenie czasu wykonywania - omijane są operacje wejścia/wyjścia. W przypadku Apache Hadoop każda operacja pośrednia jest zapisywana na dysku co skutkuje wolniejszym dostarczaniem wyników od dwóch i pół do pięciu razy. Dodatkową możliwością, która została wzięta pod uwagę podczas badań była strategia przechowywania zbioru RDD na platformie Apache Spark. Wyniki ze względu na strategię przechowywania zbioru RDD są przedstawione w tabelach: \ref{tab:reject-spark-modes-results}, \ref{tab:word-occurence-spark-modes-results}, \ref{tab:word-count-spark-modes-results}. Można zaobserwować spadek wydajności od 1,3 do 3,5 raza podczas zastosowania strategii dysku twardego. 
\section{Ocena obydwu platform}
Wybór między jednoznaczną wyższością spośród obydwu platform na podstawie wykonanych badań nie jest możliwy. Apache Spark i Apache Hadoop mają wiele zalet jak również wad. Niewątpliwą zaletą Apache Spark jest wydajność, która może być kluczowym aspektem dla wielu użytkowników. Dodatkowym czynnikiem przemawiającym za Apache Spark jest zwięzłość oraz czytelność kodu, która jest ważna wielu projektach informatycznych. Jest to bardzo ważny czynnik szczególnie w środowiskach korporacyjnych, gdzie rotacja osób rozwijających i utrzymujących systemy informatyczne jest częsta a każde niestandardowe bądź mało znane rozwiązane jest uważane za wadę ze względu na trudność w skalowaniu. Jednocześnie jest narzędziem nowym, które jeszcze nie zostało opanowane przez wielu inżynierów co może być problematyczne w początkowej fazie dużego projektu informatycznego, gdzie wymagana jest większa liczba osób pracujących nad systemem niż w fazie utrzymania. Z racji braku wielkiej ilości osób znających Apache Spark ich wynagrodzenie rośnie w górę. Taki stosunek kosztów zatrudniania oraz rekrutacji może być czynnikiem przemawiającym za wybraniem starszej ale lepiej znanej platformy - Apache Hadoop. W przypadku rozwoju aplikacji przeznaczonej na mniejszą skalę bądź na zamówienie klienta zewnętrznego bez zapewnienia utrzymania, warte jest rozważenie czy nie warto zastosować platformy nowszej, mniej znanej lecz zdecydowanie łatwiejszej w rozwijaniu - Apache Spark.
\section{Możliwości rozwoju komercyjnego aplikacji big-data-runner}
Aplikacja big-data-runner może być przydatnym narzędziem dla użytkowników bądź przedsiębiorców zajmujących się przetwarzaniem danych masowych. Aplikacja może jest na tyle elastyczna, że potrafi pobierać dane do testów wydajności z serwisu zewnętrznego jak również wykonywać operacje analizy danych masowych na już wcześniej przygotowanych danych w systemie HDFS. Pod względem komercyjnym mogłaby być rozszerzona o interfejs, który umożliwiałby użytkownikowi przesyłanie jego autorskich programów wsadowych, tak by mógł dokonywać analizy danych na podstawie swoich spersonalizowanych wymagań. Dodatkową funkcjonalnością możliwą do zaimplementowania, która mogłaby być atrakcyjna dla końcowego użytkownika jest automatyczna kalkulacja kosztów najmu chmury obliczeniowej. Taka funkcjonalność mogłaby dać użytkownikowi informację ile kosztuje analiza danych o spersonalizowanych parametrach, zdefiniowanej wielkości oraz jaki jest koszt dostawcy sprzętu, na którym jest wdrożona aplikacja. Koszty sprzętowe oraz wyniki wydajności mogą zadecydować o wybraniu jednego z dwóch narzędzi, dostawcy sprzętu komputerowego, bądź usługodawcy chmury obliczeniowej. Takie rozszerzenie było by szczególnie istotne i przydatne przy ocenie opłacalności migracji z Apache Hadoop do Apache Spark. Koszty sprzętowe korzystania z Apache Spark mogą być zbliżone podczas zastosowania strategii przechowywania danych na dysku twardym. Jeżeli taka funkcjonalność dostarczałaby wartości pieniężnych co do kosztów sprzętowych, estymacja kosztów migracji z Apache Hadoop na Apache Spark jest możliwa do zrealizowana w łatwy i szybki sposób po połączeniu z danymi kosztów rekrutacji i szkoleń inżynierów. 